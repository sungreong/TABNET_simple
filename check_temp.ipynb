{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0d3773b654528d79880d3e2b33a682f0f55f4d8370c8ead39985baabf4eb4f1db",
   "display_name": "Python 3.8.8 64-bit ('test': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "label = np.random.choice([1,2],replace=True,size=(1000,1))\n",
    "nums = np.random.normal(size=(1000,1))\n",
    "cats = np.random.choice([1,2,3,4,6],replace=True,size=(1000,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.concatenate([nums,cats,label],axis=1)\n",
    "train = pd.DataFrame(x,columns=[\"num_1\",\"cat_1\",\"target\"])\n",
    "train[\"cat_1\"] = train[\"cat_1\"].astype(\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cat_1 5\ntarget 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "nunique = train.nunique()\n",
    "types = train.dtypes\n",
    "\n",
    "categorical_columns = []\n",
    "categorical_dims =  {}\n",
    "for col in train.columns:\n",
    "    if types[col] == 'object' or nunique[col] < 200:\n",
    "        print(col, train[col].nunique())\n",
    "        l_enc = LabelEncoder()\n",
    "        train[col] = train[col].fillna(\"VV_likely\")\n",
    "        train[col] = l_enc.fit_transform(train[col].values)\n",
    "        categorical_columns.append(col)\n",
    "        categorical_dims[col] = len(l_enc.classes_)\n",
    "    else:\n",
    "        train.fillna(train[col].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unused_feat = ['temp']\n",
    "target = \"target\"\n",
    "\n",
    "features = [ col for col in train.columns if col not in unused_feat+[target]] \n",
    "\n",
    "cat_idxs = [ i for i, f in enumerate(features) if f in categorical_columns]\n",
    "\n",
    "cat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device used : cpu\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import torch\n",
    "clf = TabNetClassifier(cat_idxs=cat_idxs,\n",
    "                       cat_dims=cat_dims,\n",
    "                       cat_emb_dim=1,\n",
    "                       optimizer_fn=torch.optim.Adam,\n",
    "                       optimizer_params=dict(lr=2e-2),\n",
    "                       scheduler_params={\"step_size\":50, # how to use learning rate scheduler\n",
    "                                         \"gamma\":0.9},\n",
    "                       scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                       mask_type='entmax' # \"sparsemax\"\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[features].values\n",
    "y_train = train[target].values\n",
    "\n",
    "max_epochs = 1000 if not os.getenv(\"CI\", False) else 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0  | loss: 0.76517 | train_auc: 0.55284 |  0:00:00s\n",
      "epoch 1  | loss: 0.7731  | train_auc: 0.54458 |  0:00:00s\n",
      "epoch 2  | loss: 0.70236 | train_auc: 0.53682 |  0:00:00s\n",
      "epoch 3  | loss: 0.70894 | train_auc: 0.54361 |  0:00:00s\n",
      "epoch 4  | loss: 0.70166 | train_auc: 0.5192  |  0:00:00s\n",
      "epoch 5  | loss: 0.69576 | train_auc: 0.51546 |  0:00:00s\n",
      "epoch 6  | loss: 0.6938  | train_auc: 0.51186 |  0:00:00s\n",
      "epoch 7  | loss: 0.69338 | train_auc: 0.52758 |  0:00:00s\n",
      "epoch 8  | loss: 0.694   | train_auc: 0.53428 |  0:00:01s\n",
      "epoch 9  | loss: 0.68927 | train_auc: 0.53256 |  0:00:01s\n",
      "epoch 10 | loss: 0.68658 | train_auc: 0.55032 |  0:00:01s\n",
      "epoch 11 | loss: 0.68934 | train_auc: 0.54836 |  0:00:01s\n",
      "epoch 12 | loss: 0.69459 | train_auc: 0.55135 |  0:00:01s\n",
      "epoch 13 | loss: 0.685   | train_auc: 0.55982 |  0:00:01s\n",
      "epoch 14 | loss: 0.68631 | train_auc: 0.56228 |  0:00:01s\n",
      "epoch 15 | loss: 0.68595 | train_auc: 0.55759 |  0:00:01s\n",
      "epoch 16 | loss: 0.6888  | train_auc: 0.55071 |  0:00:01s\n",
      "epoch 17 | loss: 0.6802  | train_auc: 0.54608 |  0:00:01s\n",
      "epoch 18 | loss: 0.6841  | train_auc: 0.543   |  0:00:02s\n",
      "epoch 19 | loss: 0.68743 | train_auc: 0.55332 |  0:00:02s\n",
      "epoch 20 | loss: 0.67934 | train_auc: 0.56329 |  0:00:02s\n",
      "epoch 21 | loss: 0.68736 | train_auc: 0.57136 |  0:00:02s\n",
      "epoch 22 | loss: 0.68536 | train_auc: 0.57414 |  0:00:02s\n",
      "epoch 23 | loss: 0.68842 | train_auc: 0.57577 |  0:00:02s\n",
      "epoch 24 | loss: 0.67958 | train_auc: 0.58142 |  0:00:02s\n",
      "epoch 25 | loss: 0.68839 | train_auc: 0.58485 |  0:00:02s\n",
      "epoch 26 | loss: 0.68421 | train_auc: 0.58428 |  0:00:02s\n",
      "epoch 27 | loss: 0.68634 | train_auc: 0.57951 |  0:00:03s\n",
      "epoch 28 | loss: 0.68197 | train_auc: 0.57734 |  0:00:03s\n",
      "epoch 29 | loss: 0.68913 | train_auc: 0.5784  |  0:00:03s\n",
      "epoch 30 | loss: 0.68424 | train_auc: 0.58001 |  0:00:03s\n",
      "epoch 31 | loss: 0.68086 | train_auc: 0.58151 |  0:00:03s\n",
      "epoch 32 | loss: 0.68582 | train_auc: 0.57872 |  0:00:03s\n",
      "epoch 33 | loss: 0.68461 | train_auc: 0.5786  |  0:00:03s\n",
      "epoch 34 | loss: 0.6835  | train_auc: 0.57742 |  0:00:03s\n",
      "epoch 35 | loss: 0.6796  | train_auc: 0.5797  |  0:00:03s\n",
      "epoch 36 | loss: 0.67739 | train_auc: 0.57961 |  0:00:03s\n",
      "epoch 37 | loss: 0.68061 | train_auc: 0.57834 |  0:00:04s\n",
      "epoch 38 | loss: 0.6842  | train_auc: 0.57969 |  0:00:04s\n",
      "epoch 39 | loss: 0.68056 | train_auc: 0.57862 |  0:00:04s\n",
      "epoch 40 | loss: 0.68031 | train_auc: 0.57732 |  0:00:04s\n",
      "epoch 41 | loss: 0.68399 | train_auc: 0.5771  |  0:00:04s\n",
      "epoch 42 | loss: 0.68387 | train_auc: 0.5808  |  0:00:04s\n",
      "epoch 43 | loss: 0.68247 | train_auc: 0.58146 |  0:00:04s\n",
      "epoch 44 | loss: 0.6796  | train_auc: 0.58637 |  0:00:04s\n",
      "epoch 45 | loss: 0.68547 | train_auc: 0.58957 |  0:00:04s\n",
      "epoch 46 | loss: 0.68966 | train_auc: 0.58779 |  0:00:04s\n",
      "epoch 47 | loss: 0.67491 | train_auc: 0.58878 |  0:00:05s\n",
      "epoch 48 | loss: 0.67484 | train_auc: 0.58599 |  0:00:05s\n",
      "epoch 49 | loss: 0.67429 | train_auc: 0.58827 |  0:00:05s\n",
      "epoch 50 | loss: 0.67713 | train_auc: 0.59166 |  0:00:05s\n",
      "epoch 51 | loss: 0.67705 | train_auc: 0.59    |  0:00:05s\n",
      "epoch 52 | loss: 0.67727 | train_auc: 0.5917  |  0:00:05s\n",
      "epoch 53 | loss: 0.67806 | train_auc: 0.589   |  0:00:05s\n",
      "epoch 54 | loss: 0.6817  | train_auc: 0.59102 |  0:00:05s\n",
      "epoch 55 | loss: 0.67937 | train_auc: 0.59626 |  0:00:05s\n",
      "epoch 56 | loss: 0.67787 | train_auc: 0.59374 |  0:00:05s\n",
      "epoch 57 | loss: 0.67939 | train_auc: 0.5932  |  0:00:06s\n",
      "epoch 58 | loss: 0.68272 | train_auc: 0.59318 |  0:00:06s\n",
      "epoch 59 | loss: 0.68237 | train_auc: 0.59223 |  0:00:06s\n",
      "epoch 60 | loss: 0.67875 | train_auc: 0.59487 |  0:00:06s\n",
      "epoch 61 | loss: 0.68812 | train_auc: 0.59718 |  0:00:06s\n",
      "epoch 62 | loss: 0.67172 | train_auc: 0.59948 |  0:00:06s\n",
      "epoch 63 | loss: 0.68751 | train_auc: 0.60291 |  0:00:06s\n",
      "epoch 64 | loss: 0.67845 | train_auc: 0.60042 |  0:00:06s\n",
      "epoch 65 | loss: 0.67344 | train_auc: 0.59622 |  0:00:06s\n",
      "epoch 66 | loss: 0.6854  | train_auc: 0.5945  |  0:00:07s\n",
      "epoch 67 | loss: 0.67693 | train_auc: 0.59074 |  0:00:07s\n",
      "epoch 68 | loss: 0.67735 | train_auc: 0.58315 |  0:00:07s\n",
      "epoch 69 | loss: 0.67821 | train_auc: 0.58332 |  0:00:07s\n",
      "epoch 70 | loss: 0.69043 | train_auc: 0.58997 |  0:00:07s\n",
      "epoch 71 | loss: 0.67931 | train_auc: 0.59837 |  0:00:07s\n",
      "epoch 72 | loss: 0.68396 | train_auc: 0.60197 |  0:00:07s\n",
      "epoch 73 | loss: 0.67526 | train_auc: 0.60527 |  0:00:07s\n",
      "epoch 74 | loss: 0.67856 | train_auc: 0.60075 |  0:00:07s\n",
      "epoch 75 | loss: 0.67979 | train_auc: 0.5983  |  0:00:07s\n",
      "epoch 76 | loss: 0.68379 | train_auc: 0.59723 |  0:00:08s\n",
      "epoch 77 | loss: 0.67545 | train_auc: 0.59657 |  0:00:08s\n",
      "epoch 78 | loss: 0.68224 | train_auc: 0.59491 |  0:00:08s\n",
      "epoch 79 | loss: 0.67621 | train_auc: 0.59628 |  0:00:08s\n",
      "epoch 80 | loss: 0.67639 | train_auc: 0.59005 |  0:00:08s\n",
      "epoch 81 | loss: 0.67048 | train_auc: 0.58376 |  0:00:08s\n",
      "epoch 82 | loss: 0.67965 | train_auc: 0.5846  |  0:00:08s\n",
      "epoch 83 | loss: 0.69629 | train_auc: 0.5898  |  0:00:08s\n",
      "epoch 84 | loss: 0.67205 | train_auc: 0.59979 |  0:00:08s\n",
      "epoch 85 | loss: 0.66895 | train_auc: 0.60122 |  0:00:08s\n",
      "epoch 86 | loss: 0.67842 | train_auc: 0.59977 |  0:00:09s\n",
      "epoch 87 | loss: 0.67855 | train_auc: 0.59867 |  0:00:09s\n",
      "epoch 88 | loss: 0.67716 | train_auc: 0.59772 |  0:00:09s\n",
      "epoch 89 | loss: 0.69082 | train_auc: 0.59817 |  0:00:09s\n",
      "epoch 90 | loss: 0.68653 | train_auc: 0.60439 |  0:00:09s\n",
      "epoch 91 | loss: 0.6843  | train_auc: 0.60725 |  0:00:09s\n",
      "epoch 92 | loss: 0.6825  | train_auc: 0.61259 |  0:00:09s\n",
      "epoch 93 | loss: 0.67724 | train_auc: 0.61673 |  0:00:09s\n",
      "epoch 94 | loss: 0.67493 | train_auc: 0.61506 |  0:00:09s\n",
      "epoch 95 | loss: 0.67495 | train_auc: 0.61427 |  0:00:09s\n",
      "epoch 96 | loss: 0.67588 | train_auc: 0.6132  |  0:00:09s\n",
      "epoch 97 | loss: 0.67997 | train_auc: 0.60948 |  0:00:09s\n",
      "epoch 98 | loss: 0.66843 | train_auc: 0.61201 |  0:00:10s\n",
      "epoch 99 | loss: 0.67625 | train_auc: 0.61331 |  0:00:10s\n",
      "epoch 100| loss: 0.68892 | train_auc: 0.61287 |  0:00:10s\n",
      "epoch 101| loss: 0.67414 | train_auc: 0.61167 |  0:00:10s\n",
      "epoch 102| loss: 0.67596 | train_auc: 0.61075 |  0:00:10s\n",
      "epoch 103| loss: 0.67755 | train_auc: 0.61229 |  0:00:10s\n",
      "epoch 104| loss: 0.6653  | train_auc: 0.61492 |  0:00:10s\n",
      "epoch 105| loss: 0.67377 | train_auc: 0.61505 |  0:00:10s\n",
      "epoch 106| loss: 0.68268 | train_auc: 0.61908 |  0:00:10s\n",
      "epoch 107| loss: 0.66914 | train_auc: 0.62081 |  0:00:10s\n",
      "epoch 108| loss: 0.66925 | train_auc: 0.61959 |  0:00:10s\n",
      "epoch 109| loss: 0.6741  | train_auc: 0.61716 |  0:00:10s\n",
      "epoch 110| loss: 0.67842 | train_auc: 0.61376 |  0:00:10s\n",
      "epoch 111| loss: 0.66756 | train_auc: 0.61103 |  0:00:11s\n",
      "epoch 112| loss: 0.68239 | train_auc: 0.61003 |  0:00:11s\n",
      "epoch 113| loss: 0.67036 | train_auc: 0.60785 |  0:00:11s\n",
      "epoch 114| loss: 0.68689 | train_auc: 0.6001  |  0:00:11s\n",
      "epoch 115| loss: 0.68442 | train_auc: 0.59359 |  0:00:11s\n",
      "epoch 116| loss: 0.66903 | train_auc: 0.58724 |  0:00:11s\n",
      "epoch 117| loss: 0.68404 | train_auc: 0.58717 |  0:00:11s\n",
      "epoch 118| loss: 0.68342 | train_auc: 0.59377 |  0:00:11s\n",
      "epoch 119| loss: 0.67342 | train_auc: 0.60566 |  0:00:11s\n",
      "epoch 120| loss: 0.66242 | train_auc: 0.61091 |  0:00:11s\n",
      "epoch 121| loss: 0.67533 | train_auc: 0.61311 |  0:00:11s\n",
      "epoch 122| loss: 0.67144 | train_auc: 0.61222 |  0:00:12s\n",
      "epoch 123| loss: 0.67363 | train_auc: 0.61038 |  0:00:12s\n",
      "epoch 124| loss: 0.67013 | train_auc: 0.6095  |  0:00:12s\n",
      "epoch 125| loss: 0.6767  | train_auc: 0.61186 |  0:00:12s\n",
      "epoch 126| loss: 0.68452 | train_auc: 0.61643 |  0:00:12s\n",
      "epoch 127| loss: 0.67979 | train_auc: 0.62019 |  0:00:12s\n",
      "\n",
      "Early stopping occurred at epoch 127 with best_epoch = 107 and best_train_auc = 0.62081\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    }
   ],
   "source": [
    "clf.fit(\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    eval_set=[(X_train, y_train)],\n",
    "    eval_name=['train'],\n",
    "    eval_metric=['auc'],\n",
    "    max_epochs=max_epochs, patience=20,\n",
    "    batch_size=1024, virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    weights=1,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TabNet(\n",
       "  (embedder): EmbeddingGenerator(\n",
       "    (embeddings): ModuleList(\n",
       "      (0): Embedding(5, 1)\n",
       "    )\n",
       "  )\n",
       "  (tabnet): TabNetNoEmbeddings(\n",
       "    (initial_bn): BatchNorm1d(2, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    (encoder): TabNetEncoder(\n",
       "      (initial_bn): BatchNorm1d(2, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (initial_splitter): FeatTransformer(\n",
       "        (shared): GLU_Block(\n",
       "          (shared_layers): ModuleList(\n",
       "            (0): Linear(in_features=2, out_features=32, bias=False)\n",
       "            (1): Linear(in_features=16, out_features=32, bias=False)\n",
       "          )\n",
       "          (glu_layers): ModuleList(\n",
       "            (0): GLU_Layer(\n",
       "              (fc): Linear(in_features=2, out_features=32, bias=False)\n",
       "              (bn): GBN(\n",
       "                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (1): GLU_Layer(\n",
       "              (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "              (bn): GBN(\n",
       "                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (specifics): GLU_Block(\n",
       "          (glu_layers): ModuleList(\n",
       "            (0): GLU_Layer(\n",
       "              (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "              (bn): GBN(\n",
       "                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (1): GLU_Layer(\n",
       "              (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "              (bn): GBN(\n",
       "                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (feat_transformers): ModuleList(\n",
       "        (0): FeatTransformer(\n",
       "          (shared): GLU_Block(\n",
       "            (shared_layers): ModuleList(\n",
       "              (0): Linear(in_features=2, out_features=32, bias=False)\n",
       "              (1): Linear(in_features=16, out_features=32, bias=False)\n",
       "            )\n",
       "            (glu_layers): ModuleList(\n",
       "              (0): GLU_Layer(\n",
       "                (fc): Linear(in_features=2, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "              (1): GLU_Layer(\n",
       "                (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (specifics): GLU_Block(\n",
       "            (glu_layers): ModuleList(\n",
       "              (0): GLU_Layer(\n",
       "                (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "              (1): GLU_Layer(\n",
       "                (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): FeatTransformer(\n",
       "          (shared): GLU_Block(\n",
       "            (shared_layers): ModuleList(\n",
       "              (0): Linear(in_features=2, out_features=32, bias=False)\n",
       "              (1): Linear(in_features=16, out_features=32, bias=False)\n",
       "            )\n",
       "            (glu_layers): ModuleList(\n",
       "              (0): GLU_Layer(\n",
       "                (fc): Linear(in_features=2, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "              (1): GLU_Layer(\n",
       "                (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (specifics): GLU_Block(\n",
       "            (glu_layers): ModuleList(\n",
       "              (0): GLU_Layer(\n",
       "                (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "              (1): GLU_Layer(\n",
       "                (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): FeatTransformer(\n",
       "          (shared): GLU_Block(\n",
       "            (shared_layers): ModuleList(\n",
       "              (0): Linear(in_features=2, out_features=32, bias=False)\n",
       "              (1): Linear(in_features=16, out_features=32, bias=False)\n",
       "            )\n",
       "            (glu_layers): ModuleList(\n",
       "              (0): GLU_Layer(\n",
       "                (fc): Linear(in_features=2, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "              (1): GLU_Layer(\n",
       "                (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (specifics): GLU_Block(\n",
       "            (glu_layers): ModuleList(\n",
       "              (0): GLU_Layer(\n",
       "                (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "              (1): GLU_Layer(\n",
       "                (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (att_transformers): ModuleList(\n",
       "        (0): AttentiveTransformer(\n",
       "          (fc): Linear(in_features=8, out_features=2, bias=False)\n",
       "          (bn): GBN(\n",
       "            (bn): BatchNorm1d(2, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (selector): Entmax15()\n",
       "        )\n",
       "        (1): AttentiveTransformer(\n",
       "          (fc): Linear(in_features=8, out_features=2, bias=False)\n",
       "          (bn): GBN(\n",
       "            (bn): BatchNorm1d(2, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (selector): Entmax15()\n",
       "        )\n",
       "        (2): AttentiveTransformer(\n",
       "          (fc): Linear(in_features=8, out_features=2, bias=False)\n",
       "          (bn): GBN(\n",
       "            (bn): BatchNorm1d(2, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (selector): Entmax15()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_mapping): Linear(in_features=8, out_features=2, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "clf.network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}